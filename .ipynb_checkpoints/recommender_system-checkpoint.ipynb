{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This tutorial for recommender engine usingCollaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it uses python 3.5.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.6 :: Anaconda, Inc.\r\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing needed packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "# from pyspark.mllib.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "from time import time\n",
    "from pyspark.mllib.recommendation import MatrixFactorizationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset info\n",
    "    .Small: 100,000 ratings and 2,488 tag applications applied to 8,570 movies by 706 users. Last updated 4/2015.\n",
    "    .Full: 21,000,000 ratings and 470,000 tag applications applied to 27,000 movies by 230,000 users. Last updated 4/2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset_url = 'http://files.grouplens.org/datasets/movielens/ml-latest.zip'\n",
    "small_dataset_url = 'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining dataset location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('datasets'):\n",
    "    os.makedirs('datasets')\n",
    "cwd = os.getcwd()\n",
    "datasets_path = os.path.join(cwd, 'datasets')\n",
    "complete_dataset_path = os.path.join(datasets_path, 'ml-latest.zip')\n",
    "small_dataset_path = os.path.join(datasets_path, 'ml-latest-small.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dataset\n",
    "    . small dataset size = 955 kb \n",
    "    . complete dataset size = 264 mb\n",
    "   in this tutorial we will use the small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(small_dataset_path):\n",
    "    small_f = urllib.request.urlretrieve(small_dataset_url,small_dataset_path)\n",
    "# complete_f = urllib.request.urlretrieve (complete_dataset_url, complete_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unzip datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with zipfile.ZipFile(small_dataset_path, \"r\") as z:\n",
    "    z.extractall(datasets_path)\n",
    "\n",
    "# with zipfile.ZipFile(complete_dataset_path, \"r\") as z:\n",
    "#     z.extractall(datasets_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intializing spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "   .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "   .appName(\"Recommender-system\") \\\n",
    "   .getOrCreate()\n",
    "\n",
    "sc =  spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data set \n",
    "this tables we will use from dataset\n",
    "\n",
    "Each line in the ratings dataset (ratings.csv) is formatted as: userId,movieId,rating,timestamp\n",
    "\n",
    "Each line in the movies (movies.csv) dataset is formatted as: movieId,title,genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find ratings.csv path \n",
    "small_ratings_file = os.path.join(datasets_path, 'ml-latest-small', 'ratings.csv')\n",
    "# load ratings.csv file to dataframe \n",
    "rating_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(small_ratings_file).drop('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring that rating dataframe is loaded correctly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rating_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casting rating dataframe coloumns to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# casting userId to int\n",
    "rating_df = rating_df.withColumn('userId', rating_df['userId'].cast(IntegerType()))\n",
    "# casting movieId to int \n",
    "rating_df = rating_df.withColumn('movieId', rating_df['movieId'].cast(IntegerType()))\n",
    "#casting rating to float \n",
    "rating_df = rating_df.withColumn('rating', rating_df['rating'].cast(FloatType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print schema of rating dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parsing and removing irrelevant data from rating.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add file to rdd and removing header from it and removing timestamp from each row \n",
    "# small_ratings_data = small_ratings_raw_data.filter(lambda line: line!=small_ratings_raw_data_header)\\\n",
    "#     .map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[0],tokens[1],tokens[2])).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensuring that data loaded correctly \n",
    "    RDD format -> ('user_id','movie_id','actual rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_ratings_data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load movies.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding movies.csv file \n",
    "movies_file = os.path.join(datasets_path, 'ml-latest-small', 'movies.csv')\n",
    "# load movies.csv file to dataframe\n",
    "# rating_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(small_ratings_file).drop('timestamp')\n",
    "movies_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(movies_file).drop('genres')\n",
    "# small_movies_raw_data = sc.textFile(small_movies_file)\n",
    "# find rating .csv header \n",
    "# small_movies_raw_data_header = small_movies_raw_data.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring that movies dataframe is loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|movieId|               title|\n",
      "+-------+--------------------+\n",
      "|      1|    Toy Story (1995)|\n",
      "|      2|      Jumanji (1995)|\n",
      "|      3|Grumpier Old Men ...|\n",
      "|      4|Waiting to Exhale...|\n",
      "|      5|Father of the Bri...|\n",
      "|      6|         Heat (1995)|\n",
      "|      7|      Sabrina (1995)|\n",
      "|      8| Tom and Huck (1995)|\n",
      "|      9| Sudden Death (1995)|\n",
      "|     10|    GoldenEye (1995)|\n",
      "|     11|American Presiden...|\n",
      "|     12|Dracula: Dead and...|\n",
      "|     13|        Balto (1995)|\n",
      "|     14|        Nixon (1995)|\n",
      "|     15|Cutthroat Island ...|\n",
      "|     16|       Casino (1995)|\n",
      "|     17|Sense and Sensibi...|\n",
      "|     18|   Four Rooms (1995)|\n",
      "|     19|Ace Ventura: When...|\n",
      "|     20|  Money Train (1995)|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#casting movies id to int "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = movies_df.withColumn('movieId' ,movies_df['movieId'].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print movies_df schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parsing and removing irrelevant data from movies.csv to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add file to rdd and removing header and genere from it \n",
    "# small_movies_data = small_movies_raw_data.filter(lambda line: line!=small_movies_raw_data_header)\\\n",
    "#     .map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[0],tokens[1])).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensuring that movies.cs file is loaded correctly \n",
    "    RDD formate -> ('movie_id','movie_name(production_year)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_movies_data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing and spliting  dataset  to rdds \n",
    "    split dataset into\n",
    "        . Traning dataset = 60%\n",
    "        . Validation dataset = 20%\n",
    "        . Test Dataset = 20%\n",
    "note that we removed actual ratings from testing and validation dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split data randomly to training, validation and test data sets with ratio 60-20-20\n",
    "# training_RDD, validation_RDD, test_RDD = small_ratings_data.randomSplit([6, 2, 2], seed=0)\n",
    "training_df, validation_df, test_df = rating_df.randomSplit([0.6, 0.2, 0.2])\n",
    "# # remove actual user rating from validation dataset\n",
    "## validation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1]))\n",
    "# validation_for_predict_df = validation_df.drop(\"rating\")\n",
    "# # remove actual user rating from testing dataset\n",
    "# test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))\n",
    "# test_for_predict_df = test_df.drop(\"rating\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring and tunning our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",coldStartStrategy=\"drop\")\n",
    "\n",
    "param_grid = ParamGridBuilder().addGrid(\n",
    "    als.rank,\n",
    "    [10, 15],\n",
    ").addGrid(\n",
    "    als.maxIter,\n",
    "    [10, 15],\n",
    ").addGrid(\n",
    "    als.regParam,\n",
    "    [0.1,0.01,0.2],\n",
    ").build()\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"rating\",\n",
    ")\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=als,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    ")\n",
    "\n",
    "\n",
    "model = tvs.fit(training_df)\n",
    "\n",
    "bestmodel=model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(bestmodel\n",
    "    ._java_obj     # Get Java object\n",
    "    .parent()      # Get parent (ALS estimator)\n",
    "    .getRegParam()) \n",
    "# print ('Best Param {}:'.format(bestmodel._java_obj.parent().getRegParam()))\n",
    "# print ('Best Param {MaxIter}:'.format( bestmodel._java_obj.parent().getMaxIter()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rank=bestmodel.rank\n",
    "best_regParm=bestmodel._java_obj.parent().getRegParam()\n",
    "best_iterations=bestmodel._java_obj.parent().getMaxIter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(best_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank 4 the RMSE is 0.9033439282282294\n",
      "The best model was trained with rank 4\n",
      "For rank 8 the RMSE is 0.911418196740271\n",
      "The best model was trained with rank 4\n",
      "For rank 10 the RMSE is 0.9145175660427705\n",
      "The best model was trained with rank 4\n"
     ]
    }
   ],
   "source": [
    "# seed is used for reproducability \n",
    "# seed = 5\n",
    "iterations = 10\n",
    "# is used to avoid over and under fitting \n",
    "regularization_parameter = 0.1\n",
    "tolerance = 0.02\n",
    "ranks = [4, 8, 10]\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "best_model = None\n",
    "\n",
    "for rank in ranks:\n",
    "    als = ALS(maxIter=iterations, regParam=regularization_parameter,rank=rank, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",coldStartStrategy=\"drop\")\n",
    "    model = als.fit(training_df)\n",
    "    predictions = model.transform(validation_df)\n",
    "\n",
    "    evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                    predictionCol=\"prediction\")\n",
    "    error = evaluator.evaluate(predictions)\n",
    "\n",
    "    print ('For rank {} the RMSE is {}'.format(rank, error))\n",
    "\n",
    "    if error < min_error:\n",
    "            min_error = error\n",
    "            best_rank = rank\n",
    "            best_model = model\n",
    "\n",
    "    print ('The best model was trained with rank {}'.format(best_rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating|prediction|\n",
      "+------+-------+------+----------+\n",
      "|   474|    471|   3.0|  2.508234|\n",
      "|   462|    471|   2.5|  2.392996|\n",
      "|   555|    471|   3.0| 4.4965034|\n",
      "|   216|    471|   3.0| 3.2247887|\n",
      "|   287|    471|   4.5| 2.6791642|\n",
      "|   541|    471|   3.0|  3.425363|\n",
      "|   357|    471|   3.5|  3.823547|\n",
      "|   606|   1088|   3.0| 3.1558466|\n",
      "|   554|   1088|   5.0| 3.5911787|\n",
      "|   200|   1088|   4.0| 3.6109958|\n",
      "|   600|   1088|   3.5|  2.649283|\n",
      "|   517|   1088|   1.0|  3.050231|\n",
      "|   587|   1238|   4.0| 3.9528525|\n",
      "|   268|   1238|   5.0|  3.875886|\n",
      "|   312|   1342|   4.0|  2.720895|\n",
      "|   137|   1580|   3.5| 3.0223484|\n",
      "|   580|   1580|   4.0|  3.486576|\n",
      "|   593|   1580|   1.5| 2.4402766|\n",
      "|   606|   1580|   2.5|  3.051666|\n",
      "|    91|   1580|   3.5|  3.549459|\n",
      "+------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the BEST model for future use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = os.path.join('/home/tabdalla/Development/code/col-filtering_recommender-system', 'models', 'movie_lens_als_2')\n",
    "# best_model.save(sc, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = os.path.join('/home/tabdalla/Development/code/col-filtering_recommender-system', 'models', 'movie_lens_als_2')\n",
    "# loaded_model = MatrixFactorizationModel.load(sc, model_path)\n",
    "# predictions = loaded_model.predictAll(validation_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "# predictions.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See predected results\n",
    "    RDD format ((user_id,movie_is),predicted_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId=474, movieId=471, rating=3.0, prediction=2.5082340240478516),\n",
       " Row(userId=462, movieId=471, rating=2.5, prediction=2.392996072769165),\n",
       " Row(userId=555, movieId=471, rating=3.0, prediction=4.4965033531188965),\n",
       " Row(userId=216, movieId=471, rating=3.0, prediction=3.2247886657714844),\n",
       " Row(userId=287, movieId=471, rating=4.5, prediction=2.679164171218872)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare between predicted result and actual result in dataset\n",
    "    RDD format ((user_id,movie_id),(actual_rating,predected_rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model using test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing data the RMSE is 0.9174855406146327\n"
     ]
    }
   ],
   "source": [
    "als = ALS(maxIter=iterations, regParam=regularization_parameter,rank=rank, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",coldStartStrategy=\"drop\")\n",
    "model = als.fit(training_df)\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                    predictionCol=\"prediction\")\n",
    "error = evaluator.evaluate(predictions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model = ALS.train(training_RDD, best_rank, seed=seed, iterations=iterations,\n",
    "#                       lambda_=regularization_parameter)\n",
    "# predictions = model.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "# rates_and_preds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "# error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    \n",
    "print ('For testing data the RMSE is {}'.format(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the  model using the complete dataset instead of small data set\n",
    "  in order to get better results we shall use the complete dataset but here we used the small one due to memory limitation \n",
    "  all we need to change 'ml-latest-small' -> 'ml-latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the complete dataset file\n",
    "## TAKE CARE HERE WE CHANGED USAGE OF COMPLETE PATH TO USE SMALL DUE TO MEMORY LIMITATION \n",
    "complete_ratings_file = os.path.join(datasets_path, 'ml-latest-small', 'ratings.csv')\n",
    "# complete_ratings_raw_data = sc.textFile(complete_ratings_file)\n",
    "complete_ratings_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(complete_ratings_file).drop('timestamp')\n",
    "# complete_ratings_raw_data_header = complete_ratings_raw_data.take(1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_ratings_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse and modify ratings.csv file and parse it to rdd \n",
    "modification here means convert string results to int as we did before for accurate computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 100836 ratings in the small dataset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# complete_ratings_data = complete_ratings_raw_data.filter(lambda line: line!=complete_ratings_raw_data_header)\\\n",
    "#     .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),int(tokens[1]),float(tokens[2]))).cache()\n",
    "# casting userId to int\n",
    "complete_ratings_df = complete_ratings_df.withColumn('userId', complete_ratings_df['userId'].cast(IntegerType()))\n",
    "# casting movieId to int \n",
    "complete_ratings_df = complete_ratings_df.withColumn('movieId', complete_ratings_df['movieId'].cast(IntegerType()))\n",
    "#casting rating to float \n",
    "complete_ratings_df = complete_ratings_df.withColumn('rating', complete_ratings_df['rating'].cast(FloatType()))\n",
    "    \n",
    "print (\"There are {} ratings in the small dataset\".format((complete_ratings_df.count())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model using complete dataset with chosen parameters\n",
    "here we did't need validation dataset we need only test to find the error of our model so we splited our dataset to \n",
    "    Training -> 70%\n",
    "    test -> 30% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset to training and testing data sets with ration 70% - 30%  \n",
    "# training_RDD, test_RDD = complete_ratings_data.randomSplit([7, 3], seed=0)\n",
    "training_df, test_df = complete_ratings_df.randomSplit([0.7,0.3])\n",
    "\n",
    "# complete_model = ALS.train(training_RDD, best_rank, seed=seed, \n",
    "#                            iterations=iterations, lambda_=regularization_parameter)\n",
    "\n",
    "als = ALS(maxIter=iterations, regParam=regularization_parameter,rank=rank, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",coldStartStrategy=\"drop\")\n",
    "model = als.fit(training_df)\n",
    "#     predictions = model.transform(validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test using testing dataset to calculate error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing data the RMSE is 0.8977186728990895\n"
     ]
    }
   ],
   "source": [
    "# test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))\n",
    "\n",
    "# predictions = complete_model.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "# rates_and_preds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "# error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    \n",
    "# print ('For testing data the RMSE is {}'.format(error))\n",
    "\n",
    "# als = ALS(maxIter=iterations, regParam=regularization_parameter,rank=rank, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",coldStartStrategy=\"drop\")\n",
    "# model = als.fit(training_df)\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                    predictionCol=\"prediction\")\n",
    "error = evaluator.evaluate(predictions)\n",
    "\n",
    "print ('For testing data the RMSE is {}'.format(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "#   HERE ALSO I CHANGED THE USAGE OF COMPLETE DATASET TO SMALL ONE DUE TO MEMORY LIMITATOIN    #\n",
    "################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_movies_file = os.path.join(datasets_path, 'ml-latest-small', 'movies.csv')\n",
    "# complete_movies_raw_data = sc.textFile(complete_movies_file)\n",
    "# complete_movies_raw_data_header = complete_movies_raw_data.take(1)[0]\n",
    "complete_movies_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(complete_movies_file).drop('genres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parsing movies.csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9742movies in the complete dataset \n"
     ]
    }
   ],
   "source": [
    "complete_movies_df = complete_movies_df.withColumn('movieId' ,complete_movies_df['movieId'].cast(IntegerType()))\n",
    "print (\"There are {}movies in the complete dataset \".format(complete_movies_df.count()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate average rating for each movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|movieId|       avg(rating)|\n",
      "+-------+------------------+\n",
      "|   1580| 3.487878787878788|\n",
      "|   2366|              3.64|\n",
      "|   3175|              3.58|\n",
      "|   1088| 3.369047619047619|\n",
      "|  32460|              4.25|\n",
      "|  44022| 3.217391304347826|\n",
      "|  96488|              4.25|\n",
      "|   1238| 4.055555555555555|\n",
      "|   1342|               2.5|\n",
      "|   1591|2.6346153846153846|\n",
      "|   1645| 3.411764705882353|\n",
      "|   4519|3.3333333333333335|\n",
      "|   2142|               2.7|\n",
      "|    471|              3.55|\n",
      "|   3997|1.8333333333333333|\n",
      "|    833|               2.0|\n",
      "|   3918|3.2777777777777777|\n",
      "|   7982|              3.25|\n",
      "|   1959|3.6666666666666665|\n",
      "|  68135|              3.55|\n",
      "+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# def get_counts_and_averages(ID_and_ratings_tuple):\n",
    "#     nratings = len(ID_and_ratings_tuple[1])\n",
    "#     return ID_and_ratings_tuple[0], (nratings, float(sum(x for x in ID_and_ratings_tuple[1]))/nratings)\n",
    "\n",
    "# complete_ratings_data = complete_ratings_df.rdd.map(list)\n",
    "# # complete_ratings_df.show()\n",
    "\n",
    "movie_ID_with_avg_ratings_df=complete_ratings_df.groupby('movieId').agg({'rating':'avg'}).show()\n",
    "movies_rating_counts_df=complete_ratings_df.groupby(\"movieId\").count()\n",
    "\n",
    "\n",
    "# movie_ID_with_ratings_RDD = (complete_ratings_data.map(lambda x: (x[1], x[2])).groupByKey())\n",
    "# movie_ID_with_avg_ratings_RDD = movie_ID_with_ratings_RDD.map(get_counts_and_averages)\n",
    "# movie_ID_with_avg_ratings_RDD.collect()\n",
    "# movie_rating_counts_RDD = movie_ID_with_avg_ratings_RDD.map(lambda x: (x[0], x[1][0]))\n",
    "# movies_rating_counts_df = spark.createDataFrame(movie_rating_counts_RDD)\n",
    "# movies_rating_counts_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[movieId: int, count: bigint]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_rating_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ID_with_avg_ratings_RDD.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_rating_counts_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new user to use it in our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     0|    260|   4.0|\n",
      "|     0|      1|   3.0|\n",
      "|     0|     16|   2.0|\n",
      "|     0|     25|   3.0|\n",
      "|     0|     32|   4.0|\n",
      "|     0|    335|   4.0|\n",
      "|     0|    379|   3.0|\n",
      "|     0|    296|   2.0|\n",
      "|     0|    858|   5.0|\n",
      "|     0|     50|   3.0|\n",
      "+------+-------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[userId: int, movieId: int, rating: float]>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_user_ID = 1\n",
    "\n",
    "# The format of each line is (userID, movieID, rating)\n",
    "new_user_ratings = [\n",
    "     (0,260,4), # Star Wars (1977)\n",
    "     (0,1,3), # Toy Story (1995)\n",
    "     (0,16,2), # Casino (1995)\n",
    "     (0,25,3), # Leaving Las Vegas (1995)\n",
    "     (0,32,4), # Twelve Monkeys (a.k.a. 12 Monkeys) (1995)\n",
    "     (0,335,4), # Flintstones, The (1994)\n",
    "     (0,379,3), # Timecop (1994)\n",
    "     (0,296,2), # Pulp Fiction (1994)\n",
    "     (0,858,5) , # Godfather, The (1972)\n",
    "     (0,50,3) # Usual Suspects, The (1995)\n",
    "    ]\n",
    "\n",
    "new_user_ratings_df = spark.createDataFrame(new_user_ratings)\n",
    "\n",
    "new_user_ratings_df = new_user_ratings_df.withColumn('userId', new_user_ratings_df[0].cast(IntegerType()))\n",
    "new_user_ratings_df = new_user_ratings_df.withColumn('movieId', new_user_ratings_df[1].cast(IntegerType()))\n",
    "new_user_ratings_df = new_user_ratings_df.withColumn('rating', new_user_ratings_df[2].cast(FloatType()))\n",
    "\n",
    "new_user_ratings_df = new_user_ratings_df.selectExpr(\"userId\",\"movieId\",\"rating\")\n",
    "\n",
    "new_user_ratings_df.show()\n",
    "new_user_ratings_df.printSchema\n",
    "# df = spark.createDataFrame(new_user_ratings_df)\n",
    "# new_user_ratings_df=new_user_ratings_df.drop('_1').drop('_2').drop('_3')\n",
    "# new_user_ratings_df.drop('_1',inplace=True)\n",
    "\n",
    "# new_user_ratings_RDD = sc.parallelize(new_user_ratings)\n",
    "# new_user_ratings_df.show()\n",
    "# new_user_ratings_df.createOrReplaceTempView(new_user_ratings)\n",
    "# df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'})\n",
    "# df.rename(columns={ df.columns[1]: \"your value\" })\n",
    "# new_user_ratings_df.rename(columns={new_user_ratings.columns[1]: 'movieId'})\n",
    "# new_user_ratings_df.rename(columns={'_1': 'userId', '_2': 'movieId','_3': 'rating'},inplace= True)\n",
    "\n",
    "# column_indices = [0,1,2]\n",
    "# new_names = ['userId','movieId','rating']\n",
    "# old_names = new_user_ratings_df.columns[column_indices]\n",
    "# new_user_ratings_df.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n",
    "\n",
    "# new_user_ratings_df.printSchema\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join new user rdd with oold one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data_with_new_ratings_df = complete_ratings_df.union(new_user_ratings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId=0)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# type(complete_data_with_new_ratings_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train new ALS Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model trained in 1.745 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "als = ALS(maxIter=iterations, regParam=best_regParm,rank=best_rank, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",coldStartStrategy=\"drop\")\n",
    "new_ratings_model = als.fit(complete_data_with_new_ratings_df)\n",
    "# new_ratings_model = ALS.train(complete_data_with_new_ratings_df, best_rank, seed=seed, \n",
    "#                               iterations=iterations, lambda_=regularization_parameter)\n",
    "tt = time() - t0\n",
    "\n",
    "print (\"New model trained in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict ratings for user id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 260, 4),\n",
       " (0, 1, 3),\n",
       " (0, 16, 2),\n",
       " (0, 25, 3),\n",
       " (0, 32, 4),\n",
       " (0, 335, 4),\n",
       " (0, 379, 3),\n",
       " (0, 296, 2),\n",
       " (0, 858, 5),\n",
       " (0, 50, 3)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_user_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_user_rating' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-af6b09aa9929>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_user_rating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'new_user_rating' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[260, 1, 16, 25, 32, 335, 379, 296, 858, 50]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[movieId: int, userId: int]>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, lit\n",
    "from pyspark.sql.types import  BooleanType, StringType\n",
    "\n",
    "\n",
    "invalid_predictions = list(map(lambda x: x[1], new_user_ratings))\n",
    "print(invalid_predictions)\n",
    "#print(invalid_predictions\n",
    "#def valid_movie(movie):\n",
    "#    return not (movie in invalid_predictions)\n",
    "#valid_movie_udf = udf(valid_movie, BooleanType())\n",
    "#complete_movies_df.filter(valid_movie_udf('movieId')).show()\n",
    "\n",
    "\n",
    "user_id=new_user_ratings_df.select('userId').take(1)[0]['userId']\n",
    "\n",
    "\n",
    "new_movies_df=complete_movies_df.filter(~complete_movies_df['movieId'].isin(invalid_predictions)).withColumn('userId', lit(user_id)).drop('title')\n",
    "\n",
    "new_movies_df.printSchema\n",
    "\n",
    "\n",
    "# new_movies_df = new_movies_df.insert(0,'userId','def')\n",
    "\n",
    "# new_movies_df['ttt']= new_movies_df['moviesId']\n",
    "# new_movies_df.show()\n",
    "# new_user_ratings_df\n",
    "# new_movies_df.assign(userId=0)\n",
    "# withColumn('userId', new_user_unrated_movies_df[0].cast(IntegerType()))\n",
    "# new_movies_df['My new column'] = 'default value'\n",
    "# new_movies_df.show()\n",
    "\n",
    "\n",
    "# complete_movies_df.filter(complete_movies_df[1] not in new_user_ratings_ids)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_user_ratings[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_user_ratings_ids = map(lambda x: x[1], new_user_ratings)\n",
    "# complete_movies_data=complete_movies_df.rdd\n",
    "# new_user_unrated_movies_RDD = (complete_movies_data.filter(lambda x: x[0] not in new_user_ratings_ids).map(lambda x: (new_user_ID, x[0])))\n",
    "\n",
    "# # new_user_unrated_moves_df = complete_movies_df.\n",
    "\n",
    "# # Use the input RDD, new_user_unrated_movies_RDD, with new_ratings_model.predictAll() to predict new ratings for the movies\n",
    "# # new_user_unrated_movies_df=new_user_unrated_movies_RDD.DF\n",
    "# new_user_unrated_movies_df = spark.createDataFrame(new_user_unrated_movies_RDD)\n",
    "\n",
    "new_movies_df = new_user_unrated_movies_df.withColumn('userId', new_user_unrated_movies_df[0].cast(IntegerType()))\n",
    "new_movies_df = new_user_unrated_movies_df.withColumn('movieId', new_user_unrated_movies_df[1].cast(IntegerType()))\n",
    "\n",
    "# new_user_unrated_movies_df = new_user_unrated_movies_df.selectExpr(\"userId\",\"movieId\")\n",
    "\n",
    "# # new_user_unrated_movies_RDD.take(5)\n",
    "# new_user_unrated_movies_df.count()\n",
    "new_user_recommendations_df = new_ratings_model.transform(new_movies_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+\n",
      "|userId|movieId|prediction|\n",
      "+------+-------+----------+\n",
      "|     1|    148| 4.3277755|\n",
      "|     1|    471| 4.1512995|\n",
      "|     1|    496| 4.3277755|\n",
      "|     1|    833|  2.403976|\n",
      "|     1|   1088| 3.8973567|\n",
      "|     1|   1238| 4.5507135|\n",
      "|     1|   1342| 3.1059513|\n",
      "|     1|   1580|  4.013965|\n",
      "|     1|   1591|  3.070634|\n",
      "|     1|   1645| 3.9537828|\n",
      "|     1|   1829| 3.6319897|\n",
      "|     1|   1959| 4.2179365|\n",
      "|     1|   2122|  2.936361|\n",
      "|     1|   2142| 3.3981826|\n",
      "|     1|   2366|  4.204904|\n",
      "|     1|   2659|  2.076134|\n",
      "|     1|   2866|  4.070711|\n",
      "|     1|   3175|  4.174765|\n",
      "|     1|   3794| 2.8586311|\n",
      "|     1|   3918|   3.49266|\n",
      "+------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_user_recommendations_df.show()\n",
    "# new_movies_df.show()\n",
    "# new_user_unrated_movies_df.show(800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|movieId|count|\n",
      "+-------+-----+\n",
      "|      1|  215|\n",
      "|      2|  110|\n",
      "|      3|   52|\n",
      "|      4|    7|\n",
      "|      5|   49|\n",
      "|      6|  102|\n",
      "|      7|   54|\n",
      "|      8|    8|\n",
      "|      9|   16|\n",
      "|     10|  132|\n",
      "|     11|   70|\n",
      "|     12|   19|\n",
      "|     13|    8|\n",
      "|     14|   18|\n",
      "|     15|   13|\n",
      "|     16|   82|\n",
      "|     17|   67|\n",
      "|     18|   20|\n",
      "|     19|   88|\n",
      "|     20|   15|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_rating_counts_df.count()\n",
    "df=movies_rating_counts_df.sort('movieId')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2= new_user_recommendations_df.sort('movieId')\n",
    "# df2.withColumn('count',df[\"count\"])\n",
    "\n",
    "df3 =new_user_recommendations_df.join(movies_rating_counts_df, on='movieId')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.4'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can not merge DataFrame with instance of type <class 'pyspark.sql.dataframe.DataFrame'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-819803f189f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_user_recommendations_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# type(movies_rating_counts_df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_user_recommendations_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmovies_rating_counts_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'movieId'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/recommender-sys-env/lib/python3.5/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     59\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                          validate=validate)\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/recommender-sys-env/lib/python3.5/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             raise ValueError('can not merge DataFrame with instance of '\n\u001b[0;32m--> 524\u001b[0;31m                              'type {left}'.format(left=type(left)))\n\u001b[0m\u001b[1;32m    525\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m             raise ValueError('can not merge DataFrame with instance of '\n",
      "\u001b[0;31mValueError\u001b[0m: can not merge DataFrame with instance of type <class 'pyspark.sql.dataframe.DataFrame'>"
     ]
    }
   ],
   "source": [
    "# new_user_recommendations_df.withColumn('movie_rating_counts',movies_rating_counts_df['count'].cast(IntegerType()))\n",
    "df=new_user_recommendations_df.join(movies_rating_counts_df)\n",
    "\n",
    "type(new_user_recommendations_df)\n",
    "# type(movies_rating_counts_df)\n",
    "result=pd.merge(new_user_recommendations_df,movies_rating_counts_df,on='movieId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform new_user_recommendations_RDD into pairs of the form (Movie ID, Predicted Rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user_recommendations_rating_RDD = new_user_recommendations_RDD.map(lambda x: (x.product, x.rating))\n",
    "new_user_recommendations_rating_title_and_count_RDD = \\\n",
    "    new_user_recommendations_rating_RDD.join(complete_movies_titles).join(movie_rating_counts_RDD)\n",
    "new_user_recommendations_rating_title_and_count_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New user recommendation RDD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user_recommendations_rating_title_and_count_RDD = \\\n",
    "    new_user_recommendations_rating_title_and_count_RDD.map(lambda r: (r[1][0][1], r[1][0][0], r[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top rated movies with more than 25 ratings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_movies = new_user_recommendations_rating_title_and_count_RDD.filter(lambda r: r[2]>=25).takeOrdered(25, key=lambda x: -x[1])\n",
    "\n",
    "print ('TOP recommended movies (with more than 25 reviews):\\n%s' %\n",
    "        '\\n'.join(map(str, top_movies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict rating for user ID = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_movie = sc.parallelize([(0, 500)]) # Quiz Show (1994)\n",
    "individual_movie_rating_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD)\n",
    "individual_movie_rating_RDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model for future use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path = os.path.join(cwd, 'models', 'movie_lens_als')\n",
    "\n",
    "# Save and load model\n",
    "model.save(sc, model_path)\n",
    "same_model = MatrixFactorizationModel.load(sc, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
